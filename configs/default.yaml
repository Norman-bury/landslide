# DINOv3-LoRA-Mask2Former 滑坡检测 默认配置

# 模型配置
model:
  backbone:
    # name: "facebook/dinov3-vits16-pretrain-lvd1689m"   # HuggingFace小模型 (gated, 需网页申请访问权限)
    name: "dinov3-vitl16-pretrain-sat493m"              # 本地已下载的卫星预训练模型 (ViT-L, 300M参数)
    freeze: true                            # 冻结backbone
    image_size: 512                         # 16的整数倍
    out_indices: [5, 11, 17, 23]            # ViT-L 24层, 取4个中间层
    # out_indices: [2, 5, 8, 11]            # ViT-S 12层, 取4个中间层
    norm_type: "sat"                        # 卫星预训练用 sat, web预训练用 imagenet

  lora:
    enabled: true
    rank: 16
    alpha: 32
    dropout: 0.1
    target_modules: ["q_proj", "v_proj"]     # DINOv3 attention层的Q,V投影名称
    lora_layers: [20, 21, 22, 23]            # 只在最后4层插入LoRA (ViT-L共24层, 不配置则全插)

  segmentor:
    num_classes: 2                          # 背景 + 滑坡
    num_queries: 100                        # Mask2Former query数量
    hidden_dim: 256                         # Transformer decoder 隐藏维度
    num_decoder_layers: 9                   # Transformer decoder 层数 (官方默认 9)
    nheads: 8                               # Multi-head attention heads
    dim_feedforward: 2048                   # FFN 中间维度
    mask_dim: 256                           # Mask feature 维度
    pre_norm: false                         # 是否用 pre-LayerNorm

# 数据配置
data:
  # 数据集根目录 (相对于项目根目录)
  dataset_root: "landslide_"
  
  # Bijie 数据集 (源域/训练)
  bijie:
    root: "Bijie_landslide_dataset/Bijie-landslide-dataset"
    image_dir: "image"
    mask_dir: "mask"
    train_ratio: 0.8
    val_ratio: 0.2
  
  # Moxizhen 数据集 (目标域/测试)
  moxizhen:
    root: "moxizhen/moxizhen"
    image_dir: "img"
    label_dir: "label"
  
  # 数据加载
  image_size: 512
  batch_size: 16                            # 80GB A100 显存
  num_workers: 8

# 训练配置
training:
  epochs: 100
  optimizer:
    name: "adamw"
    lr: 1.0e-4                              # LoRA + 分割头学习率
    weight_decay: 0.01
  scheduler:
    name: "cosine"
    warmup_epochs: 5
    min_lr: 1.0e-6
  loss:
    # Mask2Former SetCriterion 权重 (Hungarian Matching)
    ce_weight: 5.0                          # 分类 loss 权重
    mask_weight: 5.0                        # Mask BCE loss 权重
    dice_weight: 5.0                        # Mask Dice loss 权重
    eos_coef: 0.1                           # no-object 类别权重
    # Hungarian Matcher cost
    cost_class: 2.0
    cost_mask: 5.0
    cost_dice: 5.0
    # Point sampling
    num_points: 12544                       # 点采样数量 (112*112)
    oversample_ratio: 3.0
    importance_sample_ratio: 0.75
  
  # 梯度累积 (低显存时使用)
  gradient_accumulation_steps: 2             # 等效 batch = 16*2 = 32
  
  amp: true                                # GPU混合精度
  early_stopping_patience: 15               # 15个epoch无提升则停止

# 评测配置
evaluation:
  metrics: ["miou", "f1", "precision", "recall"]

# 设备配置
device: "auto"                              # auto: 自动检测 cuda/mps/cpu

# 输出配置
output:
  dir: "outputs"
  save_every: 5                             # 每5个epoch保存一次
  log_every: 10                             # 每10步打印一次
